{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c668d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import Libraries and Define Paths\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from huggingface_hub import snapshot_download\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- Define File Paths Based on Your Project Structure ---\n",
    "PROCESSED_DATA_PATH = '../data/processed/filtered_complaints.csv'\n",
    "VECTOR_STORE_PATH = '../vector_store/db_faiss'\n",
    "\n",
    "# Ensure the vector store directory exists\n",
    "os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0dd754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data loaded successfully.\n",
      "Dataset shape: (363409, 8)\n",
      "Shape after dropping any remaining NaNs: (363409, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the Processed Data\n",
    "try:\n",
    "    df = pd.read_csv(PROCESSED_DATA_PATH)\n",
    "    print(\"Processed data loaded successfully.\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    # Handle potential empty narratives that might have slipped through\n",
    "    df.dropna(subset=['cleaned_narrative'], inplace=True)\n",
    "    print(f\"Shape after dropping any remaining NaNs: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file was not found at {PROCESSED_DATA_PATH}.\")\n",
    "    print(\"Please ensure you have run Task 1 successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fad6d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Text Chunking ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Narratives: 100%|██████████| 363409/363409 [02:38<00:00, 2288.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of documents created after chunking: 588437\n",
      "Example of a chunked document:\n",
      "page_content='i made the mistake of using my wellsfargo debit card to depsit funds into atm machine outside their branch i went into the branch and was told they couldnt help and had to phone the customer service for help i did this and was told i was helped gave all the info for the time terminal id aact s was able to find the transaction and give me this info he said the dispute would take a few days i waited a few days and got a letter stating my dispute was rejected i went back into and they said they never got the transaction' metadata={'complaint_id': 14061897, 'product': 'Savings Account', 'issue': 'Managing an account', 'company': 'WELLS FARGO & COMPANY', 'date_received': '2025-06-13'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 3: Text Chunking\n",
    "\n",
    "print(\"\\n--- Starting Text Chunking ---\")\n",
    "\n",
    "# We use LangChain's RecursiveCharacterTextSplitter.\n",
    "# This splitter tries to split text on a hierarchy of characters (like \"\\n\\n\", \"\\n\", \" \", \"\")\n",
    "# to keep semantically related pieces of text together as much as possible.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # The maximum size of each chunk (in characters)\n",
    "    chunk_overlap=100   # The number of characters to overlap between chunks\n",
    ")\n",
    "\n",
    "# We will create a new list to hold our chunked documents.\n",
    "# Each entry will be a LangChain 'Document' object, which holds the text\n",
    "# and its associated metadata.\n",
    "\n",
    "docs = []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Chunking Narratives\"):\n",
    "    # Split the narrative into chunks\n",
    "    chunks = text_splitter.split_text(row['cleaned_narrative'])\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # For each chunk, we create a Document object and add metadata.\n",
    "        # The metadata is crucial as it allows us to trace a retrieved chunk\n",
    "        # back to its original complaint.\n",
    "        docs.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                'complaint_id': row['Complaint ID'],\n",
    "                'product': row['Product'],\n",
    "                'issue': row['Issue'],\n",
    "                'company': row['Company'],\n",
    "                'date_received': row['Date received']\n",
    "            }\n",
    "        ))\n",
    "\n",
    "print(f\"\\nTotal number of documents created after chunking: {len(docs)}\")\n",
    "print(\"Example of a chunked document:\")\n",
    "print(docs[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eb179e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Setting up Embedding Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model 'sentence-transformers/all-MiniLM-L6-v2' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Embedding Model Setup\n",
    "\n",
    "print(\"\\n--- Setting up Embedding Model ---\")\n",
    "\n",
    "# We will use a pre-trained model from Hugging Face via the sentence-transformers library.\n",
    "# 'all-MiniLM-L6-v2' is a fantastic starting model because it's:\n",
    "# - Fast and lightweight.\n",
    "# - Provides high-quality embeddings for semantic search.\n",
    "# - Works well for general-purpose text.\n",
    "\n",
    "# LangChain provides a convenient wrapper 'HuggingFaceEmbeddings'\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "print(f\"Embedding model '{model_name}' loaded successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa336f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating and Saving the Vector Store ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create and Persist the Vector Store\n",
    "print(\"\\n--- Creating and Saving the Vector Store ---\")\n",
    "# This is the most intensive step. It will:\n",
    "# 1. Take each document chunk ('docs').\n",
    "# 2. Use the 'embeddings' model to create a vector for each chunk.\n",
    "# 3. Store the vector and its associated metadata in a FAISS index.\n",
    "\n",
    "# The `FAISS.from_documents` method handles this entire process for us.\n",
    "# NOTE: This can take several minutes and consume a significant amount of RAM.\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Save the created vector store locally.\n",
    "vector_store.save_local(VECTOR_STORE_PATH)\n",
    "\n",
    "print(f\"\\n✅ Vector store created and saved successfully at: {VECTOR_STORE_PATH}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: (Optional) Test the Vector Store\n",
    "\n",
    "print(\"\\n--- Testing the Vector Store ---\")\n",
    "\n",
    "# To verify it's working, let's load it back and perform a quick similarity search.\n",
    "db = FAISS.load_local(VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True) # allow_dangerous_deserialization is needed for FAISS with Langchain\n",
    "\n",
    "query = \"My credit card was charged for something I did not buy\"\n",
    "results = db.similarity_search(query, k=3) # k is the number of results to return\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"\\nTop 3 similar documents found:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(f\"Product: {doc.metadata.get('product', 'N/A')}\")\n",
    "    print(f\"Issue: {doc.metadata.get('issue', 'N/A')}\")\n",
    "    print(f\"Source Complaint ID: {doc.metadata.get('complaint_id', 'N/A')}\")\n",
    "    print(f\"Text Snippet: {doc.page_content[:400]}...\")\n",
    "    print(\"--------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
