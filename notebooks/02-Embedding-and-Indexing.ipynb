{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c668d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\Projects\\10 Academy\\Week-6\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import Libraries and Define Paths\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from huggingface_hub import snapshot_download\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "# --- Define File Paths Based on Your Project Structure ---\n",
    "PROCESSED_DATA_PATH = '../data/processed/filtered_complaints.csv'\n",
    "VECTOR_STORE_PATH = '../vector_store/db_faiss'\n",
    "\n",
    "# Ensure the vector store directory exists\n",
    "os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdb2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import Libraries and Define Paths\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from huggingface_hub import snapshot_download\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "# --- Define File Paths Based on Your Project Structure ---\n",
    "PROCESSED_DATA_PATH = '../data/processed/filtered_complaints.csv'\n",
    "VECTOR_STORE_PATH = '../vector_store/db_faiss'\n",
    "\n",
    "# Ensure the vector store directory exists\n",
    "os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba43e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import Libraries and Define Paths\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from huggingface_hub import snapshot_download\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "# --- Define File Paths Based on Your Project Structure ---\n",
    "PROCESSED_DATA_PATH = '../data/processed/filtered_complaints.csv'\n",
    "VECTOR_STORE_PATH = '../vector_store/db_faiss'\n",
    "\n",
    "# Ensure the vector store directory exists\n",
    "os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0dd754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data loaded successfully.\n",
      "Dataset shape: (363409, 8)\n",
      "Shape after dropping any remaining NaNs: (363409, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the Processed Data\n",
    "try:\n",
    "    df = pd.read_csv(PROCESSED_DATA_PATH)\n",
    "    print(\"Processed data loaded successfully.\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    # Handle potential empty narratives that might have slipped through\n",
    "    df.dropna(subset=['cleaned_narrative'], inplace=True)\n",
    "    print(f\"Shape after dropping any remaining NaNs: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file was not found at {PROCESSED_DATA_PATH}.\")\n",
    "    print(\"Please ensure you have run Task 1 successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad6d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Text Chunking ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Narratives: 100%|██████████| 363409/363409 [03:36<00:00, 1676.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of documents created after chunking: 588437\n",
      "Example of a chunked document:\n",
      "page_content='i made the mistake of using my wellsfargo debit card to depsit funds into atm machine outside their branch i went into the branch and was told they couldnt help and had to phone the customer service for help i did this and was told i was helped gave all the info for the time terminal id aact s was able to find the transaction and give me this info he said the dispute would take a few days i waited a few days and got a letter stating my dispute was rejected i went back into and they said they never got the transaction' metadata={'complaint_id': 14061897, 'product': 'Savings Account', 'issue': 'Managing an account', 'company': 'WELLS FARGO & COMPANY', 'date_received': '2025-06-13'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 3: Text Chunking\n",
    "\n",
    "print(\"\\n--- Starting Text Chunking ---\")\n",
    "\n",
    "# We use LangChain's RecursiveCharacterTextSplitter.\n",
    "# This splitter tries to split text on a hierarchy of characters (like \"\\n\\n\", \"\\n\", \" \", \"\")\n",
    "# to keep semantically related pieces of text together as much as possible.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # The maximum size of each chunk (in characters)\n",
    "    chunk_overlap=100   # The number of characters to overlap between chunks\n",
    ")\n",
    "\n",
    "# We will create a new list to hold our chunked documents.\n",
    "# Each entry will be a LangChain 'Document' object, which holds the text\n",
    "# and its associated metadata.\n",
    "\n",
    "docs = []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Chunking Narratives\"):\n",
    "    # Split the narrative into chunks\n",
    "    chunks = text_splitter.split_text(row['cleaned_narrative'])\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # For each chunk, we create a Document object and add metadata.\n",
    "        # The metadata is crucial as it allows us to trace a retrieved chunk\n",
    "        # back to its original complaint.\n",
    "        docs.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                'complaint_id': row['Complaint ID'],\n",
    "                'product': row['Product'],\n",
    "                'issue': row['Issue'],\n",
    "                'company': row['Company'],\n",
    "                'date_received': row['Date received']\n",
    "            }\n",
    "        ))\n",
    "\n",
    "print(f\"\\nTotal number of documents created after chunking: {len(docs)}\")\n",
    "print(\"Example of a chunked document:\")\n",
    "print(docs[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eb179e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Setting up Embedding Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yoga i7\\AppData\\Local\\Temp\\ipykernel_21320\\3910154457.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model 'sentence-transformers/all-MiniLM-L6-v2' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Embedding Model Setup\n",
    "\n",
    "print(\"\\n--- Setting up Embedding Model ---\")\n",
    "\n",
    "# We will use a pre-trained model from Hugging Face via the sentence-transformers library.\n",
    "# 'all-MiniLM-L6-v2' is a fantastic starting model because it's:\n",
    "# - Fast and lightweight.\n",
    "# - Provides high-quality embeddings for semantic search.\n",
    "# - Works well for general-purpose text.\n",
    "\n",
    "# LangChain provides a convenient wrapper 'HuggingFaceEmbeddings'\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "print(f\"Embedding model '{model_name}' loaded successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efa336f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating and Saving the Vector Store in Batches ---\n",
      "Processing the first batch to initialize the vector store...\n",
      "Initial vector store created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Batches to Vector Store: 100%|██████████| 1176/1176 [5:30:05<00:00, 16.84s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All batches have been added to the vector store.\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: '..\\\\vector_store\\\\db_faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileExistsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll batches have been added to the vector store.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# --- Step 3: Save the final, complete vector store ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVECTOR_STORE_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Vector store created and saved successfully at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVECTOR_STORE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\Projects\\10 Academy\\Week-6\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1156\u001b[39m, in \u001b[36mFAISS.save_local\u001b[39m\u001b[34m(self, folder_path, index_name)\u001b[39m\n\u001b[32m   1148\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Save FAISS index, docstore, and index_to_docstore_id to disk.\u001b[39;00m\n\u001b[32m   1149\u001b[39m \n\u001b[32m   1150\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1153\u001b[39m \u001b[33;03m    index_name: for saving with a specific index file name\u001b[39;00m\n\u001b[32m   1154\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1155\u001b[39m path = Path(folder_path)\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m \u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[38;5;66;03m# save index separately since it is not picklable\u001b[39;00m\n\u001b[32m   1159\u001b[39m faiss = dependable_faiss_import()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python313\\Lib\\pathlib\\_local.py:722\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    719\u001b[39m \u001b[33;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[32m    720\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m    724\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent == \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[31mFileExistsError\u001b[39m: [WinError 183] Cannot create a file when that file already exists: '..\\\\vector_store\\\\db_faiss'"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Creating and Saving the Vector Store in Batches ---\")\n",
    "\n",
    "# Define the size of our batches\n",
    "batch_size = 500  # A safe starting number. You can increase if you have a lot of RAM.\n",
    "\n",
    "# --- Step 1: Create the initial vector store with the first batch ---\n",
    "print(\"Processing the first batch to initialize the vector store...\")\n",
    "\n",
    "# Check if there are any documents to process\n",
    "if not docs:\n",
    "    print(\"Error: No documents to process. Please check the chunking step.\")\n",
    "else:\n",
    "    # Take the first batch of documents\n",
    "    first_batch = docs[:batch_size]\n",
    "    # Create the initial FAISS index\n",
    "    db = FAISS.from_documents(first_batch, embeddings)\n",
    "    print(\"Initial vector store created.\")\n",
    "\n",
    "    # --- Step 2: Loop through the rest of the documents in batches and add them ---\n",
    "    # We start from the end of the first batch\n",
    "    for i in tqdm(range(batch_size, len(docs), batch_size), desc=\"Adding Batches to Vector Store\"):\n",
    "        # Get the next batch of documents\n",
    "        batch = docs[i : i + batch_size]\n",
    "        # Add the documents in the current batch to the existing FAISS index\n",
    "        db.add_documents(batch)\n",
    "\n",
    "    print(\"\\nAll batches have been added to the vector store.\")\n",
    "\n",
    "    # --- Step 3: Save the final, complete vector store ---\n",
    "    db.save_local(VECTOR_STORE_PATH)\n",
    "\n",
    "    print(f\"\\n✅ Vector store created and saved successfully at: {VECTOR_STORE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad292e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Vector store created and saved successfully at: ../vector_store/db_faiss\n"
     ]
    }
   ],
   "source": [
    " # --- Step 3: Save the final, complete vector store ---\n",
    "db.save_local(VECTOR_STORE_PATH)\n",
    "\n",
    "print(f\"\\n✅ Vector store created and saved successfully at: {VECTOR_STORE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a89e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing the Vector Store ---\n",
      "Query: 'My credit card was charged for something I did not buy'\n",
      "\n",
      "Top 3 similar documents found:\n",
      "\n",
      "--- Result 1 ---\n",
      "Product: Savings Account\n",
      "Issue: Problem with a lender or other company charging your account\n",
      "Source Complaint ID: 6190201\n",
      "Text Snippet: i was charged dollars for something that i did not buy...\n",
      "--------------------\n",
      "\n",
      "--- Result 2 ---\n",
      "Product: Credit Card\n",
      "Issue: Problem with a purchase shown on your statement\n",
      "Source Complaint ID: 3117402\n",
      "Text Snippet: my card was charged with something i did not purchase i contact the credit card company and they said they were looking into it in the mean time dont do anything after a couple of months of this happening several times the issued was resolved but the credit card was reported as late payment for 120 days i tried to have it fixed with the credit card company to remove that late payment but no respon...\n",
      "--------------------\n",
      "\n",
      "--- Result 3 ---\n",
      "Product: Savings Account\n",
      "Issue: Problem with a lender or other company charging your account\n",
      "Source Complaint ID: 8612653\n",
      "Text Snippet: fraudulently charged my credit card...\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "            # Cell 6: (Optional) Test the Vector Store\n",
    "\n",
    "print(\"\\n--- Testing the Vector Store ---\")\n",
    "\n",
    "# To verify it's working, let's load it back and perform a quick similarity search.\n",
    "db = FAISS.load_local(VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True) # allow_dangerous_deserialization is needed for FAISS with Langchain\n",
    "\n",
    "query = \"My credit card was charged for something I did not buy\"\n",
    "results = db.similarity_search(query, k=3) # k is the number of results to return\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"\\nTop 3 similar documents found:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(f\"Product: {doc.metadata.get('product', 'N/A')}\")\n",
    "    print(f\"Issue: {doc.metadata.get('issue', 'N/A')}\")\n",
    "    print(f\"Source Complaint ID: {doc.metadata.get('complaint_id', 'N/A')}\")\n",
    "    print(f\"Text Snippet: {doc.page_content[:400]}...\")\n",
    "    print(\"--------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
