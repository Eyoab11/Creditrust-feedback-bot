{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80195313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install All Necessary Libraries\n",
    "\n",
    "!pip install -q -U transformers\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U langchain\n",
    "!pip install -q -U langchain-community\n",
    "!pip install -q -U sentence-transformers\n",
    "!pip install -q -U faiss-cpu #\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Mount Google Drive, Define Paths, and Unzip the Vector Store\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# --- DEFINE ALL YOUR PATHS HERE ---\n",
    "# Path to your zipped vector store in Google Drive\n",
    "ZIP_PATH = '/content/drive/MyDrive/Colab_RAG_Project/vector_store.zip'\n",
    "# Path where we will unzip the store in the Colab environment\n",
    "UNZIP_PATH = '/content/vector_store'\n",
    "# Path to the final unzipped FAISS index\n",
    "VECTOR_STORE_PATH = '/content/vector_store/vector_store/db_faiss'\n",
    "# Path to the folder where we will save our results\n",
    "OUTPUT_PATH = '/content/drive/MyDrive/Colab_RAG_Project/outputs/'\n",
    "\n",
    "# --- SETUP THE ENVIRONMENT ---\n",
    "# Create the output directory in Drive if it doesn't exist\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Unzip the vector store file for faster access\n",
    "print(f\"Unzipping {ZIP_PATH} to {UNZIP_PATH}...\")\n",
    "with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "    zip_ref.extractall(UNZIP_PATH)\n",
    "print(\"Unzipping complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Vector Store and Embedding Model\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "\n",
    "print(\"Loading vector store...\")\n",
    "db = FAISS.load_local(VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = db.as_retriever(search_kwargs={'k': 5})\n",
    "print(\"Retriever is ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d8396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Setup the LLM and RAG Chain (Final, Robust Version)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "print(\"\\nSetting up the LLM...\")\n",
    "model_id = \"microsoft/phi-2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256, temperature=0.1, top_p=0.95)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(\"LLM setup complete.\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Instruct: You are a financial analyst assistant for CrediTrust. Use ONLY the following retrieved complaint excerpts to answer the question. If the context is not enough, say that you don't have enough information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# This chain will take the 'question' and 'context' and generate an answer\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# This is the full chain that orchestrates everything\n",
    "retrieval_chain = RunnablePassthrough.assign(\n",
    "    context=(lambda x: x[\"question\"]) | retriever\n",
    ").assign(\n",
    "    answer=combine_docs_chain\n",
    ")\n",
    "\n",
    "print(\"RAG Chain is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d90d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run Evaluation and Save All Results to Google Drive (Final Version)\n",
    "# ... (the evaluation_questions list remains the same) ...\n",
    "\n",
    "# --- Define the list of questions for your evaluation ---\n",
    "evaluation_questions = [\n",
    "    \"Why are people unhappy with BNPL?\",\n",
    "    \"What are the main issues with credit card fees?\",\n",
    "    \"Are there complaints about unauthorized transactions on savings accounts?\",\n",
    "    \"Summarize the problems related to money transfers being delayed.\",\n",
    "    \"My loan application was rejected, what are common reasons mentioned in complaints?\",\n",
    "    \"Compare the top complaints for Personal Loans versus Credit Cards.\",\n",
    "    \"What do customers say about closing their accounts?\",\n",
    "    \"Is fraud a common issue in money transfer services?\"\n",
    "]\n",
    "for i, question in enumerate(evaluation_questions):\n",
    "    print(f\"\\n--- Processing Question {i+1}/{len(evaluation_questions)} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "\n",
    "    # We now pass a dictionary with the 'question' key\n",
    "    response = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "    answer = response.get('answer', 'No answer generated.')\n",
    "    context_docs = response.get('context', [])\n",
    "\n",
    "    output_content = f\"EVALUATION FOR QUESTION {i+1}\\n\"\n",
    "    output_content += \"=\"*40 + \"\\n\"\n",
    "    output_content += f\"Question: {question}\\n\"\n",
    "    output_content += \"=\"*40 + \"\\n\\n\"\n",
    "    output_content += \"Generated Answer:\\n\"\n",
    "    output_content += \"-----------------\\n\"\n",
    "    output_content += answer + \"\\n\\n\"\n",
    "    output_content += \"Retrieved Sources Used:\\n\"\n",
    "    output_content += \"-----------------------\\n\"\n",
    "\n",
    "    for j, doc in enumerate(context_docs):\n",
    "        output_content += f\"\\n--- Source {j+1} ---\\n\"\n",
    "        output_content += f\"Product: {doc.metadata.get('product', 'N/A')}\\n\"\n",
    "        output_content += f\"Text Snippet: {doc.page_content}\\n\"\n",
    "        output_content += \"------------------\\n\"\n",
    "\n",
    "    output_filename = f\"evaluation_q{i+1}_{question[:20].replace(' ', '_')}.txt\"\n",
    "    output_filepath = os.path.join(OUTPUT_PATH, output_filename)\n",
    "\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(output_content)\n",
    "\n",
    "    print(f\"âœ… Results for Question {i+1} saved to: {output_filepath}\")\n",
    "\n",
    "print(\"\\n\\nAll evaluation questions have been processed and saved to your Google Drive in the 'outputs' folder.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
